# Глубокое обучение сетей для обработки естественного языка.

>Методические указания к лабораторным занятиям по дисциплине "Машинное обучение на больших данных в информационно-аналитических системах" [Электронный ресурс] - сост. Т. А. Васяева. – Донецк: ДОННТУ, 2020.

Моделями глубокого обучения, пригодными для обработки текста 
(который можно интерпретировать как последовательности слов или 
символов), являются рекуррентные нейронные сети и одномерные сверточные 
нейронные сети.
В число прикладных применений этих алгоритмов попадают: 
1. Классификация текстов. 
2. Сравнение документов. 
3. Автоматический перевод. 
4. Анализ эмоциональной окраски. 
5. Аннотирование. 
6. Генерация текста. 
7. Чат-боты.

Обработка текста включает в себя следующие этапы: 
1. *Анализ входных данных.* Нейронная сеть работает только с числами. 
Поэтому любой текст должен пройти процесс Векторизации. 
2. *Токенизация.* Токенизация текста – разбиение текста на элементы 
(токены). В качестве токенов могут выступать буквы, слова, предложения. Для 
такой трансформации используются специальные модели, наиболее 
популярными из которых являются:

    * bag of words («cумка слов») – детальная репрезентативная модель для 
упрощения обработки текстового содержания. Она не учитывает грамматику 
или порядок слов и нужна для определения количества вхождений отдельных слов в анализируемый текст: создается вектор длиной в словарь, для каждого 
слова считается количество вхождений в текст и это число подставляется на 
соответствующую позицию в векторе. Однако, при этом теряется порядок слов 
в тексте, а значит, после векторизации предложения. 
    * n-граммы – комбинации из n последовательных терминов для 
упрощения распознавания текстового содержание. Эта модель определяет и 
сохраняет смежные последовательности слов в тексте.

3. *Векторизация.* Векторизация – замена токенов на цифры, 
сопоставление цифровых векторов с полученными ранее токенами. Существуют несколько методов кодирования: 
    - One-Hot Encoding – это метод кодирования категориальных переменных в 
числовые данные, с которыми могут работать алгоритмы машинного обучения. 
Может применяться как кодировщик каждого токена (по одному символу 
натокен) или кодировать только соответствующие токену элементы, остальные 
приравнивать к нулю.
    - Embedding – числовые векторы, которые получены из слов или других 
языковых сущностей. Каждому токену сопоставляется вектор, размерность 
которого ниже чем у One-Hot Encoding.

# Инструменты

Сегодня существует большое количество программных инструментов для создания моделей Machine Learning. Первые такие инструменты формировались в среде ученых и статистиков, где популярны языки R и Python, исторически сложились экосистемы для обработки, анализа и визуализации данных именно на этих языках, хотя определенные библиотеки машинного обучения есть и для Java, Lua, С++. При этом интерпретируемые языки программирования существенно медленнее компилируемых, поэтому на интерпретируемом языке описывают подготовку данных и структуру моделей, а основные вычисления проводят на компилируемом языке.
Будем использовать библиотеки - Tensorflow и Keras, имеющие реализацию на Python, поскольку этот язык обладает большим количеством пакетов для интеграции в разного рода сервисы и системы, а также для написания различных информационных систем.

**Tensorflow**

Библиотека, разработанная корпорацией Google для работы с тензорами, используется для построения нейросетей. Поддержка вычислений на видеокартах имеет версию для языка C++. На основе данной библиотеки строятся более высокоуровневые библиотеки для работы с нейронными сетями на уровне целых слоев. Так, некоторое время назад популярная библиотека Keras стала использовать Tensorflow как основной бэкенд для вычислений вместо аналогичной библиотеки Theano. Для работы на видеокартах NVIDIA используется библиотека cuDNN.

**Keras**

Библиотека для построения нейросетей, поддерживающая основные виды слоев и структурные элементы. Поддерживает как рекуррентные, так и сверточные нейросети, имеет в своем составе реализацию известных архитектур нейросетей (например, VGG16). Некоторое время назад слои из данной библиотеки стали доступны внутри библиотеки Tensorflow. Существуют готовые функции для работы с изображениями и текстом (Embedding слов и т.д.). Интегрирована в Apache Spark с помощью дистрибутива dist-keras.

# Анализ тотальности текста в Tensorflow и Keras

1. Данные

Будем использовать датасет, который содержит отзывы на товары с Amazon (именно на книги). Это набор 
достаточно популярный при обучении нейронным сетям и другим методам 
машинного обучения, хоть и считается немного устаревшим.

2. Подготовка к обучению

Данные были расположены в файле "all.txt", которые имели вид:
```
<review>
<unique_id>
0345467078:too_much_filler:bonner_'62
</unique_id>
<unique_id>
20951
</unique_id>
<asin>
0345467078
</asin>
<product_name>
Rage: An Alex Delaware Novel (Alex Delaware Novels (Paperback)): Books: Jonathan Kellerman
</product_name>
<product_type>
books
</product_type>
<product_type>
books
</product_type>
<helpful>
7 of 8
</helpful>
<rating>
2.0
</rating>
<title>
Too Much Filler
</title>
<date>
April 22, 2006
</date>
<reviewer>
Bonner '62
</reviewer>
<reviewer_location>
Virginia
</reviewer_location>
<review_text>
All throughtout this book Alex Delaware and his police friend Milo never stop ruminating over every new fact they uncover.  They have endless discussions on how a new tidbit might fit into the overall picture.  The reader longs for the pair to actually do something.  In the end the author walks away without even tying up all the main strings.  That is really dirty pool after making the reader wade through all the yakking.  I read and enjoyed several of the early Alex Delaware books and then quit looking for new ones, now I know why
</review_text>
</review>
```

Для анализа нам необходимы отзывы, которые заключены в </review_text></review_text>. Поэтому из файла мы извлекаем только эти данные. В программе это сделано с помощью регулярных выражений:
```python
import re
result = ' '.join(re.findall(r'<review_text>([^<>]+)</review_text>', texts_true))
```
Потом убираем переносы строк - result = result.replace('\n', ' ').

3. Листинг программы

Давайте теперь посмотрим, как можно сформировать такой тензор. Вначале загрузим тексты с отзывами.
Теперь нам нужно разбить эти высказывания на слова. Для этого воспользуемся уже знакомым инструментом Tokenizer и положим, что максимальное число слов будет равно 20000:
```python
maxWordsCount = 20000
tokenizer = Tokenizer(num_words=maxWordsCount, filters='!–"—#$%&amp;()*+,-./:;<=>?@[\\]^_`{|}~\t\n\r«»',
                         lower=True, split=' ', char_level=False)
tokenizer.fit_on_texts(result)

word_2_index = tokenizer.word_index
word_2_index.items()
```
По идее, мы здесь могли бы и не задавать максимальное число слов, тогда эта величина была бы определена автоматически при парсинге текста. Но данный параметр имеет один существенный плюс: из всех найденных слов мы оставляем 19999 наиболее часто встречаемых (maxWordsCount-1), то есть, отбрасываем редкие слова, которые особо не нужны при обучении НС.
Итак, мы разбили текст на слова и для примера выведем их начальный список с частотами появления:
```python
dist = list(tokenizer.word_counts.items())
print(dist[:10])
print(result[0][:100])
```
Далее, преобразуем текст в последовательность чисел в соответствии с полученным словарем. Для этого используется специальный метод класса Tokenizer:
```python
data = tokenizer.texts_to_sequences(result)
```
На выходе получим двумерный массив чисел объекта numpy. Теперь, нам нужно выровнять все эти векторы до длины max_text_len. Для этого используется еще один встроенный метод pad_sequences, который обрезает массив data до длины max_text_len и добавляет нули для коротких векторов:
```python
max_text_len = 190
data_pad = pad_sequences(data, maxlen=max_text_len)
print(data_pad)
print(data_pad.shape)
```
Осталось создать модель рекуррентной сети. Мы воспользуемся рекуррентным слоем LSTM. Для его создания в Keras используется класс:

*keras.layers.LSTM(units, …)*

Cоздадим LSTM-слой с 64 нейронами. Тогда размерность выходного вектора будет равна 64 элемента. Далее, добавим еще один такой же слой с 32 нейронами. У нас получится стек из двух рекуррентных слоев. На выходе поставим полносвязный слой с двумя нейронами и функцией активации softmax. Определим оптимизацию по Adam с шагом сходимости 0,0001 (одна десятитысячная):
```python
model = Sequential()
model.add(Embedding(maxWordsCount, 128, input_length = max_text_len))
model.add(LSTM(64, activation='tanh', return_sequences=True))
model.add(LSTM(32, activation='tanh'))
model.add(Dense(2, activation='softmax'))
model.summary()
 
model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam(0.0001))

history = model.fit(X_train, Y_train, batch_size=32, epochs=25, validation_data=(X_test, Y_test))

scores = model.evaluate(X_test, Y_test, verbose=1)
model.save('en-de-model.h5')
```
Сформируем какой-нибудь текст и преобразуем его во входной формат нашей сети, а также пропускаем входной вектор через сеть и на выходе получаем результат:
```python
word = 'good'

data = tokenizer.texts_to_sequences(word)
data_pad = pad_sequences(data, maxlen=max_text_len)
print('Номер слова', data) 
#print('Вектор для слова', data_pad)
model.predict(data_pad, batch_size= 5, verbose=1)
```
